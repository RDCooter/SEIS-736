package edu.stthomas.gps.project;

import java.io.IOException;
import java.text.DecimalFormat;
import java.util.HashMap;
import java.util.Map;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;
import org.apache.log4j.Logger;

/**
 * 
 * @author Robert Driesch - UST Id# 101058113
 * @version 1.0, December 1, 2014
 **/
public class DescriptionTFIDF extends Configured implements Tool {
	private static final Logger LOG = Logger.getLogger(DescriptionTFIDF.class);

	private static final String SPACE_DELIMITER = new String(" ");
	private static final String EQUALS_DELIMITER = new String("=");

	/**
	 * @version 1.0, December 1, 2014
	 **/
	public static class DescriptionTFIDFMapper extends Mapper<Text, Text, Text, Text> {

		/*
		 * Local Cache Variables for the <key,value> for reuse for each input record being processed.
		 */
		private Text xWordDescriptionKey = new Text();
		private Text xCaseNbrAndCountValue = new Text();

		/**
		 * Read the Description Word Count data and re-swizzle the key so that it is only made up of the DescriptionWord
		 * so we can determine how many times a word appears in all of the Incident Descriptions.
		 * <p>
		 * 
		 * @param aKey a composite text key that represents the DescriptionWord as well as the CaseNbr/FileName where
		 *            the word originates
		 * @param aValue the counts of the word frequency as well as the total number of times that this word occurs
		 *            within the incident description
		 * @param aContext the context object associated with this process
		 **/
		@Override
		protected void map(Text aKey, Text aValue, Context aContext) throws IOException, InterruptedException {

			// @formatter:off
			/*
			 * Expects a line of input like the following: 
			 * 		[KEY(Text(<Description_Word \t CaseNbr@FileName>))  VALUE(Text(<WordCount/AllWordCounts>))]
			 */
			 // @formatter:on

			/*
			 * Convert the key data, which is received as a Text object into a String object that we can manipulate
			 * better.
			 */
			String[] myKey = aKey.toString().split(EQUALS_DELIMITER);

			// @formatter:off
			/*
			 * Write the output record in the following format: 
			 * 		[KEY(Text(Description_Word))  VALUE(Text(<CaseNbr@FileName=WordCount/AllWordCounts>))]
			 */
			// @formatter:on
			xWordDescriptionKey.set(myKey[0]);
			xCaseNbrAndCountValue.set(myKey[1] + EQUALS_DELIMITER + aValue.toString());
			aContext.write(xWordDescriptionKey, xCaseNbrAndCountValue);
		}
	}

	/**
	 * @version 1.0, December 1, 2014
	 **/
	public static class DescriptionWordTFIDFReducer extends Reducer<Text, Text, Text, Text> {
		private static final DecimalFormat DF = new DecimalFormat("###.############");
		private static final String SLASH_DELIMITER = new String("/");

		/*
		 * Local Cache Variables for the <key, value> for reuse for each output reduce record being processed.
		 */
		private Text xTextKey = new Text();
		private Text xTextValues = new Text();

		private long xTotalDocuments;
		private Map<String, String> xAllCaseNbrWordFrequencies = new HashMap<String, String>();

		/**
		 * Read all of the <key, List(values)> pairs generated by the Mapper<> and determine the number of times that we
		 * encounter a word as it occurs within the description of the different incidents and group those counts based
		 * upon the word (key).
		 * <p>
		 * 
		 * @param aKey a Text value key that represents the word within the description of the incident
		 * @param aValues an iterable array of counts of the word found across the values for the key
		 * @param aContext the context object associated with this process
		 **/
		@Override
		protected void reduce(Text aKey, Iterable<Text> aValues, Context aContext) throws IOException,
				InterruptedException {

			/*
			 * Make sure that the temporary data structure that we use to hold the CaseNbr Word Frequencies as we sum up
			 * the totals gets cleared between different sets of iterable data.
			 */
			xAllCaseNbrWordFrequencies.clear();

			/*
			 * Convert the key which is received as a Text object into a String object that we can manipulate better.
			 */
			String myOriginalKey = aKey.toString();

			/**
			 * Loop through all of the values collected for each key (DescriptionWord) from the Mapper<> that have the
			 * pattern list like this: List[Text(<CaseNbr@FileName=WordCount/AllWordCounts>)]
			 * <p>
			 * 
			 * Split apart the CaseNbr@FileName from the WordCount Frequencies for the DescriptionWord key so that we
			 * can count the total of occurrences where this DescriptionWord appears within all of the different
			 * incidents. Store the information needed to write this record back out to the context into a temporary
			 * data structure that we can process after all of the iterable values have been read.
			 */
			int countOfDocumentsWhereWordAppears = 0;
			for (Text myValue : aValues) {
				/*
				 * Split the CaseNbr@FileName from the WordCount Frequencies using the equals inserted by the Mapper<>
				 * as a delimiter and write the values into the temporary data structure.
				 */
				String[] caseNbrAndFrequencies = myValue.toString().split(EQUALS_DELIMITER);
				xAllCaseNbrWordFrequencies.put(caseNbrAndFrequencies[0], caseNbrAndFrequencies[1]);
				countOfDocumentsWhereWordAppears++;
			}

			/*
			 * Now that we have processed all of the iterable values and generated the count of number of occurrences of
			 * this word in the different incidents, we can start to unload all of the data that we have saved away in
			 * the temporary data structure and write the formatted records back out to the context.
			 */
			StringBuilder textValueBuilder = new StringBuilder();
			for (String caseNbrKey : xAllCaseNbrWordFrequencies.keySet()) {

				/*
				 * Extract out the Word Count Frequencies from the temporary data structure.
				 */
				String[] wordFrequenciesAndTotalWords = xAllCaseNbrWordFrequencies.get(caseNbrKey).split(
						SLASH_DELIMITER);

				/*
				 * Calculate the TermFrequency (TF) quotient from the number of times this term occurs within the
				 * Incident Description (e.g. document) and the total number of terms associated with the Incident
				 * Description.
				 */
				double termFrequency = Double.valueOf(Double.valueOf(wordFrequenciesAndTotalWords[0])
						/ Double.valueOf(wordFrequenciesAndTotalWords[1]));

				/*
				 * Calculate the InverseDocumentFrequency (IDF) quotient from the total number of Incident Descriptions
				 * (e.g. document) and the number of times the word occurs in those documents.
				 */
				double inverseDocumentFrequency = (double) xTotalDocuments / (double) countOfDocumentsWhereWordAppears;

				/*
				 * Calculate the TF-IDF for the DescriptionWord, but give special attention for the cases where the word
				 * appears in all of the documents.
				 */
				double tfIdf = termFrequency * Math.log10(inverseDocumentFrequency);
//				double tfIdf = (xTotalDocuments == countOfDocumentsWhereWordAppears) ? termFrequency : termFrequency
//						* Math.log10(inverseDocumentFrequency);

				/*
				 * Build up the Format of the New Value:
				 * "[ DocumentsWithWord/TotalDocuments WordCount/AllWordCounts TF-IDF ]"
				 */
				textValueBuilder.delete(0, textValueBuilder.length());// Clear first
				textValueBuilder.append("[");
				textValueBuilder.append(SPACE_DELIMITER);
				textValueBuilder.append(String.valueOf(countOfDocumentsWhereWordAppears));
				textValueBuilder.append(SLASH_DELIMITER);
				textValueBuilder.append(String.valueOf(xTotalDocuments));
				textValueBuilder.append(SPACE_DELIMITER);
				textValueBuilder.append(wordFrequenciesAndTotalWords[0]);
				textValueBuilder.append(SLASH_DELIMITER);
				textValueBuilder.append(wordFrequenciesAndTotalWords[1]);
				textValueBuilder.append(SPACE_DELIMITER);
				textValueBuilder.append(DF.format(tfIdf));
				textValueBuilder.append(SPACE_DELIMITER);
				textValueBuilder.append("]");

				// @formatter:off
				/*
				 * Write the output record in the following format: 
				 * 		[KEY(Text(<Description_Word = CaseNbr@FileName>))  
				 * 		 	VALUE(Text(<[ DocumentsWithWord/TotalDocuments WordCount/AllWordCounts TF-IDF ]>))]
				 */
				// @formatter:on
				xTextKey.set(myOriginalKey + EQUALS_DELIMITER + caseNbrKey);
				xTextValues.set(textValueBuilder.toString());
				aContext.write(xTextKey, xTextValues);
			}
		}

		/**
		 * Setup all of the local data structures required to process the filtered NEISS data to generate the word
		 * frequency for the incident Description text.
		 * 
		 * @param aContext the context object associated with this process
		 **/
		@Override
		protected void setup(Context aContext) throws IOException, InterruptedException {
			/*
			 * Extract the Total Number of Documents to use for the TF-Test for Case Sensitivity usage while processing
			 * the incident Description words.
			 */
			xTotalDocuments = aContext.getConfiguration().getLong("wordcount.total.documents", 1);
		}
	}

	/**
	 * Setup the environment so that we can invoke the Mapper as a mapper-only job to perform the simple filtering of
	 * the initial data.
	 * 
	 * @param aArguments the arguments that were passed into the program
	 **/
	@Override
	public int run(String[] aArguments) throws Exception {
		final String usageText = "Usage: DescriptionTFIDF <input_dir> <output_dir> -totalIncidents n";

		Job job = new Job(getConf());

		/*
		 * Process any arguments passed in...
		 */
		if (aArguments.length > 0) {
			for (int i = 0; i < aArguments.length; i++) {
				if (i == 0) {
					FileInputFormat.setInputPaths(job, new Path(aArguments[i]));
				} else if (i == 1) {
					FileOutputFormat.setOutputPath(job, new Path(aArguments[i]));
				} else if ("-totalIncidents".equals(aArguments[i])) {
					i += 1; // Bump to the value
					job.getConfiguration().setLong("wordcount.total.documents", Long.parseLong(aArguments[i]));
					LOG.info("Added " + aArguments[i] + " total incidents (documents) for use in TF-IDF calculations.");
				} else {
					System.err.println("ERROR: Invalid argument : '" + aArguments[i] + "'");
					LOG.info(usageText);
					return -1;
				}
			}
		}

		job.setJarByClass(DescriptionTFIDF.class);
		job.setJobName("Calculate TF-IDF against the Incident Description Words");

		job.setMapperClass(DescriptionTFIDFMapper.class);
		job.setReducerClass(DescriptionWordTFIDFReducer.class);

		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(Text.class);

		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(Text.class);

		job.setInputFormatClass(SequenceFileInputFormat.class);
//		job.setInputFormatClass(KeyValueTextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);

		job.setNumReduceTasks(24);

		/*
		 * Run the job and wait for it to be completed.
		 */
		boolean success = job.waitForCompletion(true);
		return success ? 0 : 1;
	}

	/**
	 * The main method calls the ToolRunner.run method, which in turn calls an options parser that interprets Hadoop
	 * command-line options and puts them into a Configuration object.
	 * 
	 * @param aArguments the arguments that were passed into the program
	 **/
	public static void main(String[] aArguments) throws Exception {
		int exitCode = ToolRunner.run(new Configuration(), new DescriptionTFIDF(), aArguments);
		System.exit(exitCode);
	}
}
