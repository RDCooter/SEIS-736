package edu.stthomas.gps.project;

import java.io.BufferedReader;
import java.io.FileReader;
import java.io.IOException;
import java.net.URI;
import java.util.HashSet;
import java.util.Set;
import java.util.regex.Pattern;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.filecache.DistributedCache;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Counter;
import org.apache.hadoop.mapreduce.Counters;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Partitioner;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.StringUtils;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;
import org.apache.log4j.Logger;

/**
 * 
 * @author Robert Driesch - UST Id# 101058113
 * @version 1.0, December 1, 2014
 **/
public class KeywordFilterSortByYearCaseNbr extends Configured implements Tool {
	private static final Logger LOG = Logger.getLogger(DescriptionTFIDF.class);
	private static final Integer TOTAL_NUMBER_YEARS_OF_DATA = new Integer(17); // 1997 - 2013

	private static final String NAME_PREFIX = "/user/training/NEISS_Local/";
	private static final String KEYWORD_DATA = "Keywords.dat";

	public static enum NEISS_DATA {
		TOAL_RECORDS_PROCESSED, TOTAL_REJECTED_WORDS, NUM_REJECTED_ZEROWORDS, NUM_REJECTED_KEYWORDS, NUM_VALID_WORDS, NUM_REJECTED_DIGITS, NUM_REJECTED_NONCHARS
	}

	/**
	 * @version 1.0, December 1, 2014
	 **/
	public static class YearPartitioner extends Partitioner<Text, Text> {
		private static final int LAST_YEAR_WITH_DATA = 2013;

		// @formatter:off
		/**
		 * Read the <key, value> pair generated by the Mapper and use the year in the TreatmentDate to determine which
		 * of the Reduce Tasks this record should be routed towards. The goal is to have the data from each year routed
		 * to a different reducer so when correct number of Reduce Tasks are used, then each task should only
		 * be processing records that match the same year.
		 * <p>
		 * 
		 * Expects a line of input like the following: 
		 * 				[KEY(Text(CaseNbr))	VALUE(Text(TreatmentDate
		 * 												\tHospital
		 *		 										\tWeight
		 * 												\tStratum
		 * 												\tAge
		 * 												\tGender
		 * 												\tRace
		 * 												\tDiagnosis
		 *												\tBodyPart
		 * 												\tDisposition
		 * 												\tLocation
		 * 												\tProducts
		 * 												\tDescription))]
		 * <p>
		 * 
		 * @param aKey a simple text key that represents the CaseNbr
		 * @param aValue a tab delimited text string with the first field as the TreatmentDate
		 * @param aNumReduceTasks the number of Reduce Tasks allocated for this process
		 **/
		// @formatter:on
		@Override
		public int getPartition(Text aKey, Text aValue, int aNumReduceTasks) {
			/*
			 * Perform a sanity test to avoid any divide by zero exceptions (modulus with zero) when the number of
			 * reducers is set to zero for some reason.
			 */
			if (aNumReduceTasks == 0) {
				return 0;
			}

			/*
			 * Get the TreatmentDate from the tab delimited value in the <key,value> pair so we can extract out the year
			 * to use for the partitioning.
			 */
			String treatmentDate = aValue.toString().split("\t")[0];
			int yearInt = Integer.parseInt(treatmentDate.split("/")[2]); // Date in MDY format with "/" as the separator

			/*
			 * Set the partition number based upon the year of the incident (TreatmentDate) in order to keep the output
			 * results grouped based upon year without effecting the implicit ordering of the CaseNbr key by the
			 * map-reduce logic.
			 * 
			 * The calculation below will place the earliest year (1997) in reducer 0 and the last one (2013) in reducer
			 * 16.
			 */
			return (yearInt - LAST_YEAR_WITH_DATA + TOTAL_NUMBER_YEARS_OF_DATA - 1) % aNumReduceTasks;
		}
	}

	/**
	 * @version 1.0, December 1, 2014
	 **/
	public static class KeywordFilterMapper extends Mapper<Text, Text, Text, Text> {

		private Set<String> keywordsToSelect = new HashSet<String>();

		private static final String TAB_DELIMITER = new String("\t");
		private static final Pattern WORD_BOUNDARY = Pattern.compile("\\s*\\b\\s*");

		/**
		 * Simple mapper that will basically just read the filtered product file which is keyed by the CaseNbr and send
		 * the records to the correct reducer partitioned by the year of the TreatmentDate to keep the entries grouped
		 * by year and sorted by the CaseNbr within each year.
		 * 
		 * @param aKey a simple text key that represents the CaseNbr
		 * @param aValue a tab delimited text string with the first field as the TreatmentDate
		 * @param aContext the context object associated with this process
		 **/
		@Override
		protected void map(Text aKey, Text aValue, Context aContext) throws IOException, InterruptedException {

			// @formatter:off
			/*
			 * Expects a line of input like the following: 
			 * 				[KEY(Text(CaseNbr))	VALUE(Text(TreatmentDate
			 * 												\tHospital
			 *		 										\tWeight
			 * 												\tStratum
			 * 												\tAge
			 * 												\tGender
			 * 												\tRace
			 * 												\tDiagnosis
			 *												\tBodyPart
			 * 												\tDisposition
			 * 												\tLocation
			 * 												\tProducts
			 * 												\tDescription))]
			 */
			 // @formatter:on

			/*
			 * Convert the line of data, which is received as a Text object into a String object that we can manipulate
			 * better.
			 */
			aContext.getCounter(NEISS_DATA.TOAL_RECORDS_PROCESSED).increment(1);
			String myLine = aValue.toString();

			/*
			 * Split the line of NEISS tab delimited data into its (at most) 13 separate fields in order to grab the
			 * incident Description.
			 */
			String descriptionLine = myLine.split(TAB_DELIMITER, 13)[12];
			for (String descriptionWord : WORD_BOUNDARY.split(descriptionLine)) {
				/*
				 * Check to see if the word within the description should be included into the final set of words or if
				 * it should be skipped (because of Keyword filtering).
				 */
				if (descriptionWord.isEmpty() || !Character.isLetter(descriptionWord.charAt(0))
						|| Character.isDigit(descriptionWord.charAt(0))) {
					aContext.getCounter(NEISS_DATA.TOTAL_REJECTED_WORDS).increment(1);
					NEISS_DATA counterType;
					if (descriptionWord.isEmpty())
						counterType = NEISS_DATA.NUM_REJECTED_ZEROWORDS;
					else if (!Character.isLetter(descriptionWord.charAt(0)))
						counterType = NEISS_DATA.NUM_REJECTED_NONCHARS;
					else if (Character.isDigit(descriptionWord.charAt(0)))
						counterType = NEISS_DATA.NUM_REJECTED_DIGITS;
					else
						counterType = NEISS_DATA.NUM_REJECTED_KEYWORDS;
					aContext.getCounter(counterType).increment(1);
					continue;
				}

				/*
				 * Test to see if the word within the description matches one of the keywords so that we can allow the
				 * record to be written to the context.
				 */
				if (keywordsToSelect.contains(descriptionWord)) {
					aContext.write(aKey, aValue);
					aContext.getCounter(NEISS_DATA.NUM_VALID_WORDS).increment(1);
					break;
				}
			}

		}

		/**
		 * Setup all of the local data structures required to process the filtered NEISS data to generate the word
		 * frequency for the incident Description text.
		 * 
		 * @param aContext the context object associated with this process
		 **/
		@Override
		protected void setup(Context aContext) throws IOException, InterruptedException {

			// Get the local configuration from the context so that we can test for the different options and extract
			// out any distributed files that maybe used.
			Configuration myConfig = aContext.getConfiguration();

			/*
			 * Populate the local data structure for the distributed Keyword data.
			 */
			Path[] allCachedFiles = DistributedCache.getLocalCacheFiles(myConfig);
			parseDistributedFile(allCachedFiles[0], keywordsToSelect);
		}

		/**
		 * Read the specified distributed cache file and populate the local data structure with the contents.
		 * 
		 * @param aPath the path to the distributed cache file to use
		 * @param aCacheSet the local data structure to be updated with the contents of the file
		 **/
		private void parseDistributedFile(Path aPath, Set<String> aCacheSet) throws IOException {
			LOG.info("Added file \"" + aPath.getName().toString() + "\" from the distributed cache.");
			BufferedReader bufferedRdr = new BufferedReader(new FileReader(aPath.toString()));
			try {
				String lineOfData = bufferedRdr.readLine();
				while (lineOfData != null) {
					aCacheSet.add(lineOfData);
					lineOfData = bufferedRdr.readLine();
				}
			} catch (IOException ioe) {
				System.err.println("ERROR: Caught exception while parsing the cached file '"
						+ aPath.getName().toString() + "' : " + StringUtils.stringifyException(ioe));
			} finally {
				LOG.info(aCacheSet.size() + " entries added from distributed cache file \""
						+ aPath.getName().toString() + "\" to local data structure.");
				bufferedRdr.close();
			}
		}
	}

	/**
	 * @version 1.0, December 1, 2014
	 **/
	public static class SortByYearCaseNbrReducer extends Reducer<Text, Text, Text, Text> {

		/**
		 * Read all of the <key, List(values)> pairs generated by the Mapper<> and separate the list of values back into
		 * new records for each value in the list. This will have the effect of sorting/grouping all of the keys
		 * together and still keeping the value as a separate entry in the result file.
		 * 
		 * @param aKey a simple text key that represents the CaseNbr
		 * @param aValue a tab delimited text string with the first field as the TreatmentDate
		 * @param aContext the context object associated with this process
		 **/
		@Override
		protected void reduce(Text aKey, Iterable<Text> aValues, Context aContext) throws IOException,
				InterruptedException {

			/*
			 * Loop through all of the values collected for each key (CaseNbr) from the Mapper<> and add each value to
			 * the output as a separate <key, value> pair.
			 */
			for (Text myValue : aValues) {
				// @formatter:off
				/*
				 * Write the output record in the following format: 
				 * 		[KEY(Text(CaseNbr))	VALUE(Text(TreatmentDate
				 * 										\tHospital
				 * 										\tWeight
				 * 										\tStratum
				 * 										\tAge
				 * 										\tGender
				 * 										\tRace
				 * 										\tDiagnosis
				 *										\tBodyPart
				 * 										\tDisposition
				 * 										\tLocation
				 * 										\tProducts
				 * 										\tDescription))]
				 */
				// @formatter:on
				aContext.write(aKey, myValue);
			}
		}
	}

	/**
	 * Setup the environment so that we can invoke the Mapper as a mapper-only job to perform the simple filtering of
	 * the initial data.
	 * 
	 * @param aArguments the arguments that were passed into the program
	 **/
	@Override
	public int run(String[] aArguments) throws Exception {
		final String usageText = "Usage: KeywordFilterSortByYearCaseNbr <input_dir> <output_dir> -totalYears n";

		Job job = new Job(getConf());

		/*
		 * Process any arguments passed in...
		 */
		if (aArguments.length > 0) {
			for (int i = 0; i < aArguments.length; i++) {
				if (i == 0) {
					FileInputFormat.setInputPaths(job, new Path(aArguments[i]));
				} else if (i == 1) {
					FileOutputFormat.setOutputPath(job, new Path(aArguments[i]));
				} else if ("-totalYears".equals(aArguments[i])) {
					i += 1; // Bump to the value
					job.getConfiguration().setLong("wordcount.total.years", Integer.parseInt(aArguments[i]));
					LOG.info("Added " + aArguments[i] + " total years of data files for use in partition calculations.");
				} else {
					System.err.println("ERROR: Invalid argument : '" + aArguments[i] + "'");
					LOG.info(usageText);
					return -1;
				}
			}
		}

		job.setJarByClass(KeywordFilterSortByYearCaseNbr.class);
		job.setJobName("Keyword Filter and Sort NEISS Data by Year & CaseNbr");

		job.setMapperClass(KeywordFilterMapper.class);
		job.setReducerClass(SortByYearCaseNbrReducer.class);
		job.setPartitionerClass(YearPartitioner.class);

		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(Text.class);

		job.setInputFormatClass(KeyValueTextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);

		job.setNumReduceTasks(job.getConfiguration().getInt("wordcount.total.years", TOTAL_NUMBER_YEARS_OF_DATA));

		DistributedCache.addCacheFile(new URI(NAME_PREFIX + KEYWORD_DATA), job.getConfiguration());
		LOG.info("Added file to the distributed cache: " + NAME_PREFIX + KEYWORD_DATA);

		/*
		 * Run the job and wait for it to be completed.
		 */
		boolean success = job.waitForCompletion(true);

		/*
		 * Quickly output the local counters to the local output stream (console).
		 */
		Counters allCounters = job.getCounters();
		Counter myCounter = allCounters.findCounter(NEISS_DATA.TOAL_RECORDS_PROCESSED);
		LOG.info(myCounter.getDisplayName() + " : " + myCounter.getValue());
		myCounter = allCounters.findCounter(NEISS_DATA.NUM_VALID_WORDS);
		LOG.info(myCounter.getDisplayName() + " : " + myCounter.getValue());
		myCounter = allCounters.findCounter(NEISS_DATA.TOTAL_REJECTED_WORDS);
		LOG.info(myCounter.getDisplayName() + " : " + myCounter.getValue());
		myCounter = allCounters.findCounter(NEISS_DATA.NUM_REJECTED_ZEROWORDS);
		LOG.info(myCounter.getDisplayName() + " : " + myCounter.getValue());
		myCounter = allCounters.findCounter(NEISS_DATA.NUM_REJECTED_KEYWORDS);
		LOG.info(myCounter.getDisplayName() + " : " + myCounter.getValue());
		myCounter = allCounters.findCounter(NEISS_DATA.NUM_REJECTED_DIGITS);
		LOG.info(myCounter.getDisplayName() + " : " + myCounter.getValue());
		myCounter = allCounters.findCounter(NEISS_DATA.NUM_REJECTED_NONCHARS);
		LOG.info(myCounter.getDisplayName() + " : " + myCounter.getValue());

		return success ? 0 : 1;
	}

	/**
	 * The main method calls the ToolRunner.run method, which in turn calls an options parser that interprets Hadoop
	 * command-line options and puts them into a Configuration object.
	 * 
	 * @param aArguments the arguments that were passed into the program
	 **/
	public static void main(String[] aArguments) throws Exception {
		int exitCode = ToolRunner.run(new Configuration(), new KeywordFilterSortByYearCaseNbr(), aArguments);
		System.exit(exitCode);
	}
}
